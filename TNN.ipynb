{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing libraries ###\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Author: Harideep Nair ###\n",
    "### Excitatory Column (EC) ###\n",
    "\n",
    "    # Consists of 'q' excitatory neurons with 'p' synapses each.\n",
    "    # Implements an SRM0 neuron model.\n",
    "    \n",
    "    # Args: tin_max - Maximum input spiketime.\n",
    "    #       p       - Number of synapses per neuron.\n",
    "    #       q       - Number of neurons.\n",
    "    #       wres    - Bit resolution for synaptic weights.\n",
    "    #       theta   - Excitation threshold for neuron. An output spike is generated when neuron's body potential reaches\n",
    "    #                 this threshold.\n",
    "    #       ntype   - Type of neuron response function. Supports step-no-leak and ramp-no-leak response functions.\n",
    "    #       w_init  - Type of initialization for synaptic weights. Supports 'zero', 'random uniform' and\n",
    "    #                 'random normal' initializations.\n",
    "    #       ramp    - Slope of ramp for ramp-no-leak response function. Default is 1.\n",
    "\n",
    "class ExcitatoryColumn():\n",
    "    def __init__(self, tin_max, p, q, wres, theta, ntype=\"rnl\", w_init=\"zero\", ramp=1):\n",
    "        self.p             = p\n",
    "        self.q             = q\n",
    "        self.wmax          = 2**wres-1\n",
    "        self.theta         = theta\n",
    "        self.ntype         = ntype\n",
    "        self.ramp          = ramp\n",
    "        \n",
    "        # Synaptic weight initialization. Shape of weights is [self.q,self.p].\n",
    "        if w_init       == \"zero\":\n",
    "            self.weights       = torch.zeros(self.q, self.p)\n",
    "        elif w_init     == \"uniform\":\n",
    "            self.weights       = torch.randint(low=0, high=self.wmax+1, size=(self.q, self.p)).type(torch.FloatTensor)\n",
    "        elif w_init     == \"normal\":\n",
    "            self.weights       = torch.round(((self.wmax+1)/2+torch.randn(self.q, self.p)).clamp_(0,self.wmax))\n",
    "        \n",
    "        # Calculates length of the time dimension (self.time) required for each response function model.\n",
    "        # self.time relates to the maximum output spike time (say, tout_max). In fact, self.time = tout_max + 1.\n",
    "        if self.ntype   == \"snl\":\n",
    "            self.time      = tin_max + 1\n",
    "        elif self.ntype == \"rnl\":\n",
    "            self.time      = tin_max + self.wmax\n",
    "            \n",
    "        self.pot           = torch.zeros(self.time, self.q)\n",
    "        self.ec_spiketimes = float('Inf')*torch.ones(self.q)\n",
    "        self.const         = torch.arange(self.time).repeat(self.q, self.p, 1).permute(2,0,1)\n",
    "    \n",
    "    \n",
    "    # Implements a step-no-leak response function model.\n",
    "    \n",
    "        # Args: input_spiketimes - Tensor of input spiketimes with shape [self.p].\n",
    "        # Returns 1) a tensor of output spiketimes with shape [self.q].\n",
    "        #         2) a tensor of body potentials with shape [self.time, self.q].\n",
    "    \n",
    "    def StepNoLeak(self, input_spiketimes):\n",
    "        weights                                             = self.weights.repeat(self.time, 1, 1)\n",
    "        spikes                                              = input_spiketimes.repeat(self.time, self.q, 1)\n",
    "        spikes                                              = self.const - spikes\n",
    "        spikes[spikes>=0]                                   = 1\n",
    "        spikes[spikes<0]                                    = 0\n",
    "        responses                                           = torch.mul(spikes, weights)\n",
    "        pot                                                 = torch.sum(responses, dim=2)\n",
    "        \n",
    "        temp                                                = pot.clone()\n",
    "        temp[temp<self.theta]                               = 0\n",
    "        temp[temp>=self.theta]                              = 1\n",
    "        tempsum                                             = torch.sum(temp, dim=0)\n",
    "        \n",
    "        ec_spiketimes                                       = self.time - tempsum\n",
    "        ec_spiketimes[ec_spiketimes == self.time]           = float('Inf')\n",
    "        return ec_spiketimes, pot\n",
    "    \n",
    "    \n",
    "    # Implements a ramp-no-leak response function model.\n",
    "    \n",
    "        # Args: input_spiketimes - Tensor of input spiketimes with shape [self.p].\n",
    "        # Returns 1) a tensor of output spiketimes with shape [self.q].\n",
    "        #         2) a tensor of body potentials with shape [self.time, self.q].\n",
    "    \n",
    "    def RampNoLeak(self, input_spiketimes, ramp=1):\n",
    "        weights                                             = self.weights.repeat(self.time, 1, 1)\n",
    "        spikes                                              = input_spiketimes.repeat(self.time, self.q, 1)\n",
    "        spikes                                              = self.const - spikes\n",
    "        spikes[spikes>=0]                                   = 1\n",
    "        spikes[spikes<0]                                    = 0\n",
    "        responses                                           = ramp * torch.cumsum(spikes,dim=0)\n",
    "        responses[responses>=weights]                       = weights[responses>=weights]\n",
    "        pot                                                 = torch.sum(responses, dim=2)\n",
    "        \n",
    "        temp                                                = pot.clone()\n",
    "        temp[temp<self.theta]                               = 0\n",
    "        temp[temp>=self.theta]                              = 1\n",
    "        tempsum                                             = torch.sum(temp, dim=0)\n",
    "        \n",
    "        ec_spiketimes                                       = self.time - tempsum\n",
    "        ec_spiketimes[ec_spiketimes == self.time]           = float('Inf')\n",
    "        return ec_spiketimes, pot\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        if self.ntype == \"snl\":\n",
    "            #step-no-leak response\n",
    "            self.ec_spiketimes, self.pot = self.StepNoLeak(data)\n",
    "            \n",
    "        elif self.ntype == \"rnl\":\n",
    "            #ramp-no-leak response\n",
    "            self.ec_spiketimes, self.pot = self.RampNoLeak(data, self.ramp)\n",
    "            \n",
    "        return self.ec_spiketimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lateral Inhibition (LI) ###\n",
    "\n",
    "# Implements 1-WTA with lowest index tie-breaking\n",
    "\n",
    "class LateralInhibition():\n",
    "        \n",
    "    def __call__(self,ec_out, k=1):\n",
    "        wintime                      = torch.min(ec_out)\n",
    "        if wintime != float('Inf'):\n",
    "            sort_times, sort_idx     = torch.sort(ec_out)\n",
    "            win_times, win_idx       = sort_times[:k], sort_idx[:k]\n",
    "            li_out                   = float('Inf')*torch.ones(ec_out.shape)\n",
    "            li_out[win_idx]          = win_times\n",
    "        else:\n",
    "            li_out                   = ec_out\n",
    "            win_idx                  = -1\n",
    "\n",
    "        return li_out, win_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unsupervised Spike Timing Dependent Plasticity (STDP) - Modified ###\n",
    "\n",
    "# Modification of baseline RSTDP - implements much more stochasticity.\n",
    "# Each synapse has a separate Bernoulli random variable associated with it.\n",
    "\n",
    "class STDP():\n",
    "    def __init__(self, wres):\n",
    "        self.wmax       = 2**(wres)-1\n",
    "        \n",
    "    def __call__(self, intimes, outtimes, weights, rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown):\n",
    "        intimes         = torch.flatten(intimes)\n",
    "        q               = outtimes.shape[0]\n",
    "        p               = intimes.shape[0]\n",
    "        ec_in           = intimes.repeat(q,1)\n",
    "        li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "\n",
    "        # Case 1 (capture)\n",
    "        weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                       += rvcapture[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                        * torch.max(rvmin[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)], \\\n",
    "                          torch.diagonal(rvstickup[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                          [:,weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)].long()],0))\n",
    "        \n",
    "        # Case 2 (minus)\n",
    "        weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                       -= rvcapture[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                        * torch.max(rvmin[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)], \\\n",
    "                          torch.diagonal(rvstickdown[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                          [:,weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)].long()],0))\n",
    "        \n",
    "        # Case 3 (search)\n",
    "        weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                       += rvsearch[(ec_in!=float('Inf'))*(li_out==float('Inf'))]\n",
    "        \n",
    "        # Case 4 (backoff)\n",
    "        weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                       -= rvbackoff[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                        * torch.max(rvmin[(ec_in==float('Inf'))*(li_out!=float('Inf'))], \\\n",
    "                          torch.diagonal(rvstickdown[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                          [:,weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))].long()],0))\n",
    "        \n",
    "        return weights.clamp(0, self.wmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Author: Harideep Nair ###\n",
    "### Spike Timing Dependent Plasticity (STDP) with Reward for TNN Column ###\n",
    "\n",
    "# Adds partial reinforcement to baseline unsupervised STDP.\n",
    "# If the winner neuron's index matches the desired index corresponding to the assigned label, then proceed with\n",
    "# conventional STDP cases except for search.\n",
    "# Else if the winner neuron does not correspond to the assigned label, do reverse STDP for capture case with\n",
    "# backoff probability. Backoff and minus cases are not required. Search is executed as usual.\n",
    "\n",
    "    # Args: wres          - Bit resolution for weights.\n",
    "    #       stochasticity - Decides how correlated or uncorrelated weight updates will be, for a column's synapses. Could\n",
    "    #                       be low or high.\n",
    "    #       layer         - Determines if STDP is done for a single cloumn (laye = 0) or on a layer level (layer = 1).\n",
    "    #       reward        - Reward signal to guide STDP to learn the desired output label assignments.\n",
    "    #                       When the winning neuron index matches the desired label, reward is '1' -> perform conventional STDP.\n",
    "    #                       When there is no winning neuron, reward is '0' -> just perform search.\n",
    "    #                       When the winning neuron index doesn't match the desired label, reward is '-1' -> perform anti-STDP.\n",
    "    #       intimes       - Tensor of input spiketimes with shape [1,in_channels,height,width]. However, the actual shape\n",
    "    #                       doesn't matter since it's flattened later. After flattening, the shape becomes [self.p].\n",
    "    #       outtimes      - Tensor of LI's output spiketimes with shape [self.q].\n",
    "    #       weights       - Tensor of synaptic weights with shape [self.q,self.p].\n",
    "    #       rvcapture     - BRV for 'capture' and 'minus' cases.\n",
    "    #       rvsearch      - BRV for 'search' case.\n",
    "    #       rvbackoff     - BRV for 'backoff' case.\n",
    "    #       rvmin         - BRV for enforcing a minimum probability of update.\n",
    "    #       rvstickup     - BRV for sticking the weights towards wmax. Helps in generating a bimodal weight distribution.\n",
    "    #       rvstickdown   - BRV for sticking the weights towards 0. Helps in generating a bimodal weight distribution.\n",
    "    # Returns a tensor of RSTDP-updated synaptic weights of shape [self.q,self.p].\n",
    "\n",
    "class RSTDP():\n",
    "    def __init__(self, wres, stochasticity=\"high\", layer=0):\n",
    "        self.wmax           = 2**(wres)-1\n",
    "        self.stoch          = stochasticity\n",
    "        self.layer          = layer\n",
    "        \n",
    "    def __call__(self, reward, intimes, outtimes, weights, rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown):\n",
    "        if self.layer == 0:\n",
    "            intimes         = torch.flatten(intimes)\n",
    "            q               = outtimes.shape[0]\n",
    "            p               = intimes.shape[0]\n",
    "            ec_in           = intimes.repeat(q,1)\n",
    "            li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "        elif self.layer == 1:\n",
    "            ec_in           = intimes\n",
    "            li_out          = outtimes\n",
    "        \n",
    "        # Low stochasticity - All Bernoulli random variables are shared across the entire column.\n",
    "        if self.stoch == \"low\":\n",
    "            \n",
    "            if reward == 1:\n",
    "                # Case 1 (capture)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                               += rvcapture * torch.max(rvmin, rvstickup \\\n",
    "                                  [weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)].long()])\n",
    "                \n",
    "                # Case 2 (minus)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                               -= rvcapture * torch.max(rvmin, rvstickdown \\\n",
    "                                  [weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)].long()])\n",
    "                \n",
    "                # Case 4 (backoff)\n",
    "                weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                               -= rvbackoff * torch.max(rvmin, rvstickdown \\\n",
    "                                  [weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))].long()])\n",
    "                \n",
    "            elif reward == 0:\n",
    "                \n",
    "                # Case 3 (search)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                               += rvsearch\n",
    "                \n",
    "            elif reward == -1:\n",
    "                # Case 1 (capture)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                               -= rvbackoff * torch.max(rvmin, rvstickdown \\\n",
    "                                  [weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)].long()])\n",
    "\n",
    "                # Case 3 (search)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                               += rvsearch\n",
    "                \n",
    "        # High stochasticity - Each synapse has a separate Bernoulli random variable associated with it.  \n",
    "        elif self.stoch == \"high\":\n",
    "            \n",
    "            if reward == 1:\n",
    "                # Case 1 (capture)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                               += rvcapture[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                                * torch.max(rvmin[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)], \\\n",
    "                                  torch.diagonal(rvstickup[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                                  [:,weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)].long()],0))\n",
    "\n",
    "                # Case 2 (minus)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                               -= rvcapture[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                                * torch.max(rvmin[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)], \\\n",
    "                                  torch.diagonal(rvstickdown[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                                  [:,weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)].long()],0))\n",
    "\n",
    "                # Case 4 (backoff)\n",
    "                weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                               -= rvbackoff[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                                * torch.max(rvmin[(ec_in==float('Inf'))*(li_out!=float('Inf'))], \\\n",
    "                                  torch.diagonal(rvstickdown[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                                  [:,weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))].long()],0))\n",
    "                \n",
    "            elif reward == 0:\n",
    "\n",
    "                # Case 3 (search)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                               += rvsearch[(ec_in!=float('Inf'))*(li_out==float('Inf'))]\n",
    "                \n",
    "            elif reward == -1:\n",
    "                # Case 1 (capture)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                               -= rvbackoff[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                                * torch.max(rvmin[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)], \\\n",
    "                                  torch.diagonal(rvstickdown[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                                  [:,weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)].long()],0))\n",
    "\n",
    "                # Case 3 (search)\n",
    "                weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                               += rvsearch[(ec_in!=float('Inf'))*(li_out==float('Inf'))]\n",
    "        \n",
    "        return weights.clamp_(0, self.wmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDP_Det():\n",
    "    def __init__(self, wres):\n",
    "        self.wmax       = 1\n",
    "        \n",
    "    def __call__(self, intimes, outtimes, weights, lr_capture, lr_backoff, lr_search,\n",
    "                   recc, recc_start):\n",
    "        intimes         = torch.flatten(intimes)\n",
    "        q               = outtimes.shape[0]\n",
    "        p               = intimes.shape[0]\n",
    "        ec_in           = intimes.repeat(q,1)\n",
    "        li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "\n",
    "        # Case 1 (capture)\n",
    "        weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                       += lr_capture\n",
    "        \n",
    "        # Case 2 (minus)\n",
    "        weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                       -= lr_capture\n",
    "        \n",
    "        # Case 3 (search)\n",
    "        if recc:\n",
    "            #don't search the recurrent_layer\n",
    "            weights[:,:recc_start][((ec_in!=float('Inf'))*(li_out==float('Inf')))[:,:recc_start]] \\\n",
    "                           += lr_search\n",
    "        else:\n",
    "            weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                           += lr_search\n",
    "        \n",
    "        # Case 4 (backoff)\n",
    "        if recc:\n",
    "            #only backoff for the recurrent_layer\n",
    "            weights[:,recc_start:][((ec_in==float('Inf'))*(li_out!=float('Inf')))[:,recc_start:]] \\\n",
    "                        -= lr_backoff\n",
    "        else:\n",
    "            weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                           -= lr_backoff\n",
    "            \n",
    "        weights[:,:recc_start] = weights[:,:recc_start].clamp(0, self.wmax)\n",
    "        weights[:,recc_start:] = weights[:,recc_start:].clamp(0, 0.5)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSTDP_Det():\n",
    "    def __init__(self, wres, layer=0):\n",
    "        self.wmax           = 1\n",
    "        self.layer          = layer\n",
    "        \n",
    "    def __call__(self, reward, intimes, outtimes, weights, lr_capture, lr_backoff, lr_search, \\\n",
    "                recc, recc_start):\n",
    "        if self.layer == 0:\n",
    "            intimes         = torch.flatten(intimes)\n",
    "            q               = outtimes.shape[0]\n",
    "            p               = intimes.shape[0]\n",
    "            ec_in           = intimes.repeat(q,1)\n",
    "            li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "        elif self.layer == 1:\n",
    "            ec_in           = intimes\n",
    "            li_out          = outtimes\n",
    "        \n",
    "        if reward == 1:\n",
    "            # Case 1 (capture)\n",
    "            weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                               += lr_capture\n",
    "                \n",
    "            # Case 2 (minus)\n",
    "            weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                               -= lr_capture\n",
    "                \n",
    "            # Case 4 (backoff)\n",
    "            if recc:\n",
    "                #only backoff for the recurrent_layer\n",
    "                weights[:,recc_start:][((ec_in==float('Inf'))*(li_out!=float('Inf')))[:,recc_start:]] \\\n",
    "                            -= lr_backoff\n",
    "            else:\n",
    "                weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                               -= lr_backoff\n",
    "                \n",
    "        elif reward == 0:\n",
    "                \n",
    "            # Case 3 (search)\n",
    "            if recc:\n",
    "                #don't search the recurrent_layer\n",
    "                weights[:,:recc_start][((ec_in!=float('Inf'))*(li_out==float('Inf')))[:,:recc_start]] \\\n",
    "                               += lr_search\n",
    "            else:\n",
    "                weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                               += lr_search\n",
    "                \n",
    "        elif reward == -1:\n",
    "            # Case 1 (capture)\n",
    "            weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                               -= lr_capture\n",
    "\n",
    "            # Case 3 (search)\n",
    "            if recc:\n",
    "                #don't search the recurrent_layer\n",
    "                weights[:,:recc_start][((ec_in!=float('Inf'))*(li_out==float('Inf')))[:,:recc_start]] \\\n",
    "                               += lr_search\n",
    "            else:\n",
    "                weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                               += lr_search\n",
    "                \n",
    "        return weights.clamp_(0, self.wmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDP_Det_TMod():\n",
    "    def __init__(self, wres, tmax):\n",
    "        self.wmax       = 1\n",
    "        self.tmax       = tmax\n",
    "        \n",
    "    def __call__(self, intimes, outtimes, weights, lr_capture, lr_capture_min, \\\n",
    "                 lr_minus, lr_minus_min, lr_backoff, lr_search,\n",
    "                   recc, recc_start):\n",
    "        ###experiment\n",
    "        intimes[recc_start:][intimes[recc_start:] != float('inf')] -= 1\n",
    "        ####\n",
    "        \n",
    "        intimes         = torch.flatten(intimes)\n",
    "        q               = outtimes.shape[0]\n",
    "        p               = intimes.shape[0]\n",
    "        ec_in           = intimes.repeat(q,1)\n",
    "        li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "\n",
    "        capture         = torch.linspace(lr_capture_min, lr_capture, self.tmax + 1)\n",
    "        minus           = torch.linspace(lr_minus_min, lr_minus, self.tmax + 1)\n",
    "        \n",
    "        # Case 1 (capture)\n",
    "        if ((li_out - ec_in)[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)]).shape[0] > 0:\n",
    "            weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)] \\\n",
    "                       += capture[((li_out - ec_in)[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)]).long()]\n",
    "        \n",
    "        # Case 2 (minus)\n",
    "        if ((ec_in - li_out)[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)]).shape[0] > 0:\n",
    "            weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)] \\\n",
    "                       -= minus[((ec_in - li_out)[(ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)]).long()]\n",
    "        \n",
    "        if recc:\n",
    "            #don't search the recurrent_layer\n",
    "            weights[:,:recc_start][((ec_in!=float('Inf'))*(li_out==float('Inf')))[:,:recc_start]] \\\n",
    "                                   += lr_search\n",
    "        else:\n",
    "            weights[(ec_in!=float('Inf'))*(li_out==float('Inf'))] \\\n",
    "                                   += lr_search\n",
    "            \n",
    "        \n",
    "        # Case 4 (backoff)\n",
    "        if recc:\n",
    "            weights[:,:recc_start][((ec_in==float('Inf'))*(li_out!=float('Inf')))[:,:recc_start]] \\\n",
    "                        -= lr_backoff\n",
    "            #only backoff for the recurrent_layer\n",
    "            weights[:,recc_start:][((ec_in==float('Inf'))*(li_out!=float('Inf')))[:,recc_start:]] \\\n",
    "                        -= lr_backoff * 800\n",
    "        else:\n",
    "            weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                           -= lr_backoff\n",
    "        return weights.clamp(0, self.wmax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class STDP_Det_TMod():\n",
    "#     def __init__(self, wres, tmax):\n",
    "#         self.wmax       = 1\n",
    "#         self.tmax       = tmax\n",
    "        \n",
    "#     def __call__(self, intimes, outtimes, weights, lr_capture, lr_minus, lr_backoff, lr_search,\n",
    "#                    recc, recc_start):\n",
    "#         intimes         = torch.flatten(intimes)\n",
    "#         q               = outtimes.shape[0]\n",
    "#         p               = intimes.shape[0]\n",
    "#         ec_in           = intimes.repeat(q,1)\n",
    "#         li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "\n",
    "#         capture         = torch.linspace(lr_capture - 0.003, lr_capture, self.tmax + 1)\n",
    "#         minus           = torch.linspace(lr_minus - 0.003, lr_minus, self.tmax + 1)\n",
    "        \n",
    "#         case1 = (ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)\n",
    "#         case2 = (ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)\n",
    "#         case3 = (ec_in!=float('Inf'))*(li_out==float('Inf'))\n",
    "#         case4 = (ec_in==float('Inf'))*(li_out!=float('Inf'))\n",
    "        \n",
    "#         # Case 1 (capture)\n",
    "#         if ((li_out - ec_in)[case1]).shape[0] > 0:\n",
    "#             weights[case1] += capture[((li_out - ec_in)[case1]).long()]\n",
    "        \n",
    "#         # Case 2 (minus)\n",
    "#         if (((ec_in - li_out)[:,:recc_start])[case2[:,:recc_start]]).shape[0] > 0:\n",
    "#             weights[:,:recc_start][case2[:,:recc_start]] \\\n",
    "#                        -= minus[(((ec_in - li_out)[:,:recc_start])[case2[:,:recc_start]]).long()]\n",
    "              \n",
    "#         # Case 2 (minus) recurrent\n",
    "#         if (((ec_in - li_out)[:,recc_start:])[case2[:,recc_start:]]).shape[0] > 0:\n",
    "#             weights[:,recc_start:][case2[:,recc_start:]] \\\n",
    "#                        -= capture[(((ec_in - li_out)[:,recc_start:])[case2[:,recc_start:]]).long()]\n",
    "#         if recc:\n",
    "#             #don't search the recurrent_layer\n",
    "#             weights[:,:recc_start][case3[:,:recc_start]] \\\n",
    "#                                    += lr_search\n",
    "            \n",
    "# #             if outtimes[outtimes != float('inf')].shape[0] > 0:\n",
    "# #                 #remove from the recc layer\n",
    "# #                 weights[:,recc_start:][case3[:,recc_start:]] \\\n",
    "# #                                        -= lr_search\n",
    "#         else:\n",
    "#             weights[case3] \\\n",
    "#                                    += lr_search\n",
    "            \n",
    "        \n",
    "#         # Case 4 (backoff)\n",
    "#         if recc:\n",
    "#             weights[:,:recc_start][case4[:,:recc_start]] \\\n",
    "#                         -= lr_backoff\n",
    "#             #only backoff for the recurrent_layer\n",
    "#             weights[:,recc_start:][case4[:,recc_start:]] \\\n",
    "#                         -= lr_backoff * 800\n",
    "#         else:\n",
    "#             weights[case4] \\\n",
    "#                            -= lr_backoff\n",
    "            \n",
    "#         weights[:,:recc_start] = (weights[:,:recc_start]).clamp(0, self.wmax)\n",
    "#         weights[:,recc_start:] = (weights[:,recc_start:]).clamp(0, 0.75)\n",
    "#         return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSTDP_Det_TMod():\n",
    "    def __init__(self, wres, tmax):\n",
    "        self.wmax       = 1\n",
    "        self.tmax       = tmax\n",
    "        \n",
    "    def __call__(self, reward, target, intimes, outtimes, weights, lr_capture, lr_backoff, lr_search):\n",
    "        intimes         = torch.flatten(intimes)\n",
    "        q               = outtimes.shape[0]\n",
    "        p               = intimes.shape[0]\n",
    "        ec_in           = intimes.repeat(q,1)\n",
    "        li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "\n",
    "        capture         = torch.linspace(lr_capture - 0.008, lr_capture, self.tmax + 1)\n",
    "        \n",
    "        \n",
    "        if reward == 1:\n",
    "            # Case 1 (capture)\n",
    "            weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                               += capture[((ec_in)[(ec_in!=float('Inf'))*(li_out!=float('Inf'))]).long()]\n",
    "                \n",
    "            weights[(ec_in==float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                               -= lr_backoff \n",
    "        \n",
    "        elif reward == -1:\n",
    "            # Case 1 (capture)\n",
    "            weights[(ec_in!=float('Inf'))*(li_out!=float('Inf'))] \\\n",
    "                               -= capture[((ec_in)[(ec_in!=float('Inf'))*(li_out!=float('Inf'))]).long()]\n",
    "\n",
    "            weights[int(target)][(ec_in!=float('Inf'))[int(target)]] \\\n",
    "                               += lr_search\n",
    "        \n",
    "        elif reward == 0:  \n",
    "            weights[int(target)][(ec_in!=float('Inf'))[int(target)]] \\\n",
    "                               += lr_search * 10\n",
    "        return weights.clamp(0, self.wmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class STDP_Det_SMod():\n",
    "#     def __init__(self, wres, tmax):\n",
    "#         self.wmax       = 1\n",
    "#         self.tmax       = tmax\n",
    "        \n",
    "#     def __call__(self, intimes, outtimes, weights, lr_capture,lr_capture_min, lr_minus,lr_minus_min, \\\n",
    "#                  lr_backoff, lr_search, recc, recc_start):\n",
    "#         intimes         = torch.flatten(intimes)\n",
    "#         q               = outtimes.shape[0]\n",
    "#         p               = intimes.shape[0]\n",
    "#         ec_in           = intimes.repeat(q,1)\n",
    "#         li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "\n",
    "#         capture         = torch.linspace(lr_capture_min, lr_capture, self.tmax + 1)\n",
    "#         minus           = torch.linspace(lr_minus_min, lr_minus, self.tmax + 1)\n",
    "        \n",
    "#         case1 = (ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)\n",
    "#         case2 = (ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)\n",
    "#         case3 = (ec_in!=float('Inf'))*(li_out==float('Inf'))\n",
    "#         case4 = (ec_in==float('Inf'))*(li_out!=float('Inf'))\n",
    "        \n",
    "        \n",
    "#         #Case 1\n",
    "#         if ((li_out - ec_in)[case1]).shape[0] > 0:\n",
    "#             weights[case1] \\\n",
    "#                        += capture[((li_out - ec_in)[case1]).long()]\n",
    "        \n",
    "#         #Case 2\n",
    "#         if ((ec_in - li_out)[case2]).shape[0] > 0:\n",
    "#             weights[case2] \\\n",
    "#                        -= minus[((ec_in - li_out)[case2]).long()]\n",
    "        \n",
    "#         #Case 3 backoff in search\n",
    "#         if outtimes[outtimes != float('inf')].shape[0] > 0:\n",
    "#             weights[case3] -= lr_backoff\n",
    "        \n",
    "#         #Case 4 backoff\n",
    "#         weights[case4] -= lr_backoff\n",
    "        \n",
    "        \n",
    "#         return weights.clamp(0, self.wmax)\n",
    "\n",
    "\n",
    "\n",
    "class STDP_Det_SMod():\n",
    "    def __init__(self, wres, tmax):\n",
    "        self.wmax       = 1\n",
    "        self.tmax       = tmax\n",
    "        \n",
    "    def __call__(self, intimes, outtimes, winner, weights, lr_capture, lr_capture_min, \\\n",
    "                 lr_minus, lr_minus_min, lr_backoff, lr_search,\n",
    "                   recc, recc_start):\n",
    "        intimes         = torch.flatten(intimes)\n",
    "        q               = outtimes.shape[0]\n",
    "        p               = intimes.shape[0]\n",
    "        ec_in           = intimes.repeat(q,1)\n",
    "        li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "        \n",
    "        case1 = (ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)\n",
    "        case2 = (ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)\n",
    "        case3 = (ec_in!=float('Inf'))*(li_out==float('Inf'))\n",
    "        case4 = (ec_in==float('Inf'))*(li_out!=float('Inf'))\n",
    "        \n",
    "        #Case 1\n",
    "        weights[case1] += lr_capture\n",
    "        \n",
    "        #Case 2\n",
    "        weights[case2] -= lr_minus\n",
    "        \n",
    "        #Case 3 backoff only for the winner\n",
    "        if winner != -1:\n",
    "            case5 = case1[winner.item()].repeat(q,1)\n",
    "            case5[winner.item()] = False\n",
    "            \n",
    "            weights[case5] -= lr_backoff\n",
    "        \n",
    "        #case 4 backoff\n",
    "        weights[case4] -= lr_backoff\n",
    "        \n",
    "        if outtimes[outtimes != float('inf')].shape[0] == 0:\n",
    "            weights[case3] += lr_search\n",
    "        \n",
    "        return weights.clamp(0, self.wmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDP_Det_Recc():\n",
    "    def __init__(self, wres, tmax):\n",
    "        self.wmax       = 1\n",
    "        self.tmax       = tmax\n",
    "        \n",
    "    def __call__(self, intimes, outtimes, winner, weights, lr_capture, lr_capture_min, \\\n",
    "                 lr_minus, lr_minus_min, lr_backoff, lr_search,\n",
    "                   recc, recc_start):\n",
    "        intimes         = torch.flatten(intimes)\n",
    "        q               = outtimes.shape[0]\n",
    "        p               = intimes.shape[0]\n",
    "        ec_in           = intimes.repeat(q,1)\n",
    "        li_out          = outtimes.repeat(p,1).permute(1,0)\n",
    "        \n",
    "        case1 = (ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in<=li_out)\n",
    "        case2 = (ec_in!=float('Inf'))*(li_out!=float('Inf'))*(ec_in>li_out)\n",
    "        case3 = (ec_in!=float('Inf'))*(li_out==float('Inf'))\n",
    "        case4 = (ec_in==float('Inf'))*(li_out!=float('Inf'))\n",
    "        \n",
    "        \n",
    "        weights[:,recc_start:][case1[:,recc_start:]] += lr_capture\n",
    "        weights[:,recc_start:][case4[:,recc_start:]] -= lr_backoff\n",
    "        weights[:,recc_start:][case3[:,recc_start:]] -= lr_backoff\n",
    "        \n",
    "        return weights.clamp(0, self.wmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
