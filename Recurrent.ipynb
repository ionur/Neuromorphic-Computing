{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from TNNColumn.ipynb\n",
      "importing Jupyter notebook from TNN.ipynb\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "duplicate argument 'lr_capture' in function definition (<string>, line 50)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/ilaydaonur/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-1-a8e2a3b28e9c>\"\u001b[0m, line \u001b[1;32m22\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from TNNColumn import *\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m991\u001b[0m, in \u001b[1;35m_find_and_load\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m975\u001b[0m, in \u001b[1;35m_find_and_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m655\u001b[0m, in \u001b[1;35m_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m618\u001b[0m, in \u001b[1;35m_load_backward_compatible\u001b[0m\n",
      "  File \u001b[1;32m\"/Users/ilaydaonur/opt/anaconda3/lib/python3.8/site-packages/import_ipynb.py\"\u001b[0m, line \u001b[1;32m61\u001b[0m, in \u001b[1;35mload_module\u001b[0m\n    exec(code, mod.__dict__)\n",
      "  File \u001b[1;32m\"<string>\"\u001b[0m, line \u001b[1;32m21\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m991\u001b[0m, in \u001b[1;35m_find_and_load\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m975\u001b[0m, in \u001b[1;35m_find_and_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m655\u001b[0m, in \u001b[1;35m_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m618\u001b[0m, in \u001b[1;35m_load_backward_compatible\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/ilaydaonur/opt/anaconda3/lib/python3.8/site-packages/import_ipynb.py\"\u001b[0;36m, line \u001b[0;32m61\u001b[0;36m, in \u001b[0;35mload_module\u001b[0;36m\u001b[0m\n\u001b[0;31m    exec(code, mod.__dict__)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m duplicate argument 'lr_capture' in function definition\n"
     ]
    }
   ],
   "source": [
    "### Importing libraries ###\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "import import_ipynb\n",
    "from TNNColumn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentTNN():\n",
    "    def __init__(self, res_arr, size_arr, stdp_arr, theta_arr, winit_arr, ntype, rv = None, lr = None ):\n",
    "        self.input_size            = size_arr[0]\n",
    "        self.hidden_size           = size_arr[1]\n",
    "        self.out_size              = size_arr[2]\n",
    "        self.hidden_stdp           = stdp_arr[0]            #first layer is stdp\n",
    "        self.out_stdp              = stdp_arr[1]\n",
    "        self.stdp_random           = stdp_arr[2]\n",
    "        self.theta_hidden          = theta_arr[0]\n",
    "        self.theta_out             = theta_arr[1]\n",
    "        self.winit_hidden          = winit_arr[0]\n",
    "        self.winit_out             = winit_arr[1]\n",
    "        self.ntype                 = ntype\n",
    "        self.tmax_hidden           = res_arr[0]\n",
    "        self.tmax_out              = res_arr[1]\n",
    "        self.wres                  = res_arr[2]\n",
    "        self.wmax                  = 2**(self.wres)-1\n",
    "        self.rv                    = rv\n",
    "        self.lr                    = lr\n",
    "    \n",
    "        #last out_size rows correspond to the recurrent connections from the output back to the\n",
    "        #hidden layer\n",
    "        self.recurrent_size    = self.hidden_size\n",
    "        \n",
    "        self.layer1_input_size = self.input_size + self.recurrent_size \n",
    "        self.layer1_out_size   = self.hidden_size\n",
    "        self.layer2_input_size = self.layer1_out_size\n",
    "        self.layer2_out_size   = self.out_size\n",
    "        \n",
    "        \n",
    "        #create the columns\n",
    "        self.hidden_layer      = TNNColumn(  self.tmax_hidden, \n",
    "                                             self.layer1_input_size,  \n",
    "                                             self.layer1_out_size, \n",
    "                                             self.wres, \n",
    "                                             self.theta_hidden, \n",
    "                                             self.ntype, \n",
    "                                             self.hidden_stdp,\n",
    "                                             self.stdp_random,\n",
    "                                             self.winit_hidden)\n",
    "        self.out_layer         = TNNColumn(  self.tmax_out, \n",
    "                                             self.layer2_input_size,  \n",
    "                                             self.layer2_out_size,    \n",
    "                                             self.wres, \n",
    "                                             self.theta_out,    \n",
    "                                             self.ntype, \n",
    "                                             self.out_stdp,\n",
    "                                             self.stdp_random,\n",
    "                                             self.winit_out)\n",
    "        \n",
    "    def generate_rv(self, datasize, seq_len, p, q, layer):\n",
    "        start       = time.time()\n",
    "        \n",
    "        ucapture    = self.rv[layer][0]\n",
    "        usearch     = self.rv[layer][1]\n",
    "        ubackoff    = self.rv[layer][2]\n",
    "        umin        = self.rv[layer][3]\n",
    "        \n",
    "        bcapture    = Bernoulli(ucapture/1024)\n",
    "        bsearch     = Bernoulli(usearch/1024)\n",
    "        bbackoff    = Bernoulli(ubackoff/1024)\n",
    "        bmin        = Bernoulli(umin/1024)\n",
    "        \n",
    "        w           = torch.Tensor([float(l) for l in range(self.wmax+1)])\n",
    "        bstickup    = Bernoulli((w/self.wmax)*(2-w/self.wmax))\n",
    "        bstickdown  = Bernoulli((1-w/self.wmax)*(1+w/self.wmax))\n",
    "\n",
    "        rvcapture   = bcapture.sample(  [datasize, seq_len, q, p] )\n",
    "        rvsearch    = bsearch.sample(   [datasize, seq_len, q, p] )\n",
    "        rvbackoff   = bbackoff.sample(  [datasize, seq_len, q, p] )\n",
    "        rvmin       = bmin.sample(      [datasize, seq_len, q, p] )\n",
    "        rvstickup   = bstickup.sample(  [datasize, seq_len, q, p] )\n",
    "        rvstickdown = bstickdown.sample([datasize, seq_len, q, p] )\n",
    "\n",
    "        end         = time.time()\n",
    "        print(\"Random variables generated in \", end-start)\n",
    "        \n",
    "        return (rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown)\n",
    "    \n",
    "    def train(self, train_loader, datasize, seq_len, epoch, layer, k_hidden):\n",
    "        if self.stdp_random:\n",
    "            rvcapture1, rvsearch1, rvbackoff1, rvmin1, rvstickup1, rvstickdown1 = self.generate_rv(datasize, seq_len, self.layer1_input_size, self.hidden_size, 0)\n",
    "            rvcapture2, rvsearch2, rvbackoff2, rvmin2, rvstickup2, rvstickdown2 = self.generate_rv(datasize, seq_len, self.layer2_input_size, self.out_size   , 1)\n",
    "\n",
    "            for i in range(epoch):\n",
    "                start = time.time()\n",
    "                for inx, (data, label) in enumerate(train_loader):\n",
    "                    print(\"Iteration: {0}\\r\".format(inx), end=\"\") \n",
    "                    rvcapture   = [rvcapture1[inx],   rvcapture2[inx]   ]\n",
    "                    rvsearch    = [rvsearch1[inx],    rvsearch2[inx]    ]\n",
    "                    rvbackoff   = [rvbackoff1[inx],   rvbackoff2[inx]   ]\n",
    "                    rvmin       = [rvmin1[inx],       rvmin2[inx]       ]\n",
    "                    rvstickup   = [rvstickup1[inx],   rvstickup2[inx]   ]\n",
    "                    rvstickdown = [rvstickdown1[inx], rvstickdown2[inx] ]\n",
    "                    \n",
    "                    if layer == 'hidden':\n",
    "                        self.train_hidden_layer_random(data[0], label, rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown)\n",
    "                    elif layer == 'out':\n",
    "                        self.train_output_layer_random(data[0], label, rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown)\n",
    "                    elif layer == 'both':\n",
    "                        self.train_both_layers_random(data[0], label, rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown)\n",
    "                    endt                   = time.time()\n",
    "                    print(\"                                 Time elapsed: {0}\\r\".format(endt-start), end=\"\")\n",
    "                end   = time.time()\n",
    "                print(\"Column training done in \", end-start)\n",
    "        else:\n",
    "            self.lr_capture     = self.lr[0]\n",
    "            self.lr_capture_min = self.lr[1]\n",
    "            self.lr_minus       = self.lr[2]\n",
    "            self.lr_minus_min   = self.lr[3]\n",
    "            self.lr_backoff     = self.lr[4]\n",
    "            self.lr_search      = self.lr[5]\n",
    "            prev_weights = torch.zeros((self.layer1_out_size, self.layer1_input_size))\n",
    "            for i in range(epoch):\n",
    "                start = time.time()\n",
    "                for inx, (data, label) in enumerate(train_loader):\n",
    "                    print(\"Iteration: {0}\\r\".format(inx), end=\"\") \n",
    "                    if layer == 'hidden':\n",
    "                        self.train_hidden_layer_det(data[0], label, k_hidden)\n",
    "                    elif layer == 'out':\n",
    "                        self.train_output_layer_det(data[0], label, k_hidden)\n",
    "                    elif layer == 'both':\n",
    "                        self.train_both_layers_det(data[0], label, k_hidden)\n",
    "                    endt                   = time.time()\n",
    "                    print(\"                                 Time elapsed: {0}\\r\".format(endt-start), end=\"\")\n",
    "                    print(\"                                                              Conv: {0}\\r\".format( \\\n",
    "                                torch.sum(torch.abs(prev_weights - self.hidden_layer.ec.weights ))), end=\"\")\n",
    "                    prev_weights = self.hidden_layer.ec.weights.clone()\n",
    "                end   = time.time()\n",
    "                print(\"Column training done in \", end-start)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #trains one sample\n",
    "    def train_both_layers_random(self, sample, target, rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown):        \n",
    "        #initial hidden state is 0\n",
    "        out_prev = torch.ones(self.recurrent_size) * float('inf')\n",
    "        \n",
    "        for inx, data in enumerate(sample):\n",
    "            #concatenate input with the previous output\n",
    "            data = torch.cat((data, out_prev))\n",
    "            # First layer feedforward processing\n",
    "            out1, winner            = self.hidden_layer(data)\n",
    "            \n",
    "            #first layer is regular stdp\n",
    "            self.hidden_layer.ec.weights    = self.hidden_layer.stdp(data, out1, self.hidden_layer.ec.weights, \\\n",
    "                                                         rvcapture[0][inx], rvsearch[0][inx], rvbackoff[0][inx], \\\n",
    "                                                         rvmin[0][inx], rvstickup[0][inx], rvstickdown[0][inx])\n",
    "            \n",
    "            #update previous out\n",
    "            out_prev               = out1 \n",
    "            \n",
    "            #second layer feedforward processing\n",
    "            out2, winner            = self.out_layer(out1)\n",
    "            if self.out_stdp == 'stdp':\n",
    "                self.out_layer.ec.weights = self.out_layer.stdp(out1, out2, self.out_layer.ec.weights, \\\n",
    "                                                         rvcapture[1][inx], rvsearch[1][inx], rvbackoff[1][inx], \\\n",
    "                                                         rvmin[1][inx], rvstickup[1][inx], rvstickdown[1][inx])\n",
    "            elif self.out_stdp == 'rstdp':\n",
    "                reward = 0\n",
    "                if winner != -1:\n",
    "                    if target == winner.item():\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        reward = -1\n",
    "                self.out_layer.ec.weights = self.out_layer.stdp(reward, out1, out2, self.out_layer.ec.weights, \\\n",
    "                                                         rvcapture[1][inx], rvsearch[1][inx], rvbackoff[1][inx], \\\n",
    "                                                         rvmin[1][inx], rvstickup[1][inx], rvstickdown[1][inx])\n",
    "            \n",
    "            \n",
    "    def train_hidden_layer_random(self, sample, target, rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown):\n",
    "        #initial hidden state is 0\n",
    "        out_prev = torch.ones(self.recurrent_size) * float('inf')\n",
    "        for inx, data in enumerate(sample):\n",
    "            r = data[data != float('inf')]\n",
    "            if r.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            #concatenate input with the previous output\n",
    "            data = torch.cat((data, out_prev))\n",
    "            \n",
    "            #feedforward pass\n",
    "            out, winner            = self.hidden_layer(data)\n",
    "            \n",
    "            #hidden layer is regular stdp\n",
    "            self.hidden_layer.ec.weights    = self.hidden_layer.stdp(data, out, self.hidden_layer.ec.weights, \\\n",
    "                                                         rvcapture[0][inx], rvsearch[0][inx], rvbackoff[0][inx], \\\n",
    "                                                         rvmin[0][inx], rvstickup[0][inx], rvstickdown[0][inx])\n",
    "            #update previous out\n",
    "            out_prev               = out\n",
    "        \n",
    "    def train_output_layer_random(self, sample, target, rvcapture, rvsearch, rvbackoff, rvmin, rvstickup, rvstickdown):\n",
    "        for inx, data in enumerate(sample):\n",
    "            \n",
    "            #feedforward pass\n",
    "            out, winner            = self.out_layer(data)\n",
    "            \n",
    "            if self.out_stdp == 'stdp':\n",
    "                self.out_layer.ec.weights = self.out_layer.stdp(data, out, self.out_layer.ec.weights, \\\n",
    "                                                         rvcapture[1][inx], rvsearch[1][inx], rvbackoff[1][inx], \\\n",
    "                                                         rvmin[1][inx], rvstickup[1][inx], rvstickdown[1][inx])\n",
    "            elif self.out_stdp == 'rstdp':\n",
    "                reward = 0\n",
    "                if winner != -1:\n",
    "                    if target == winner.item():\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        reward = -1\n",
    "                self.out_layer.ec.weights = self.out_layer.stdp(reward, data, out, self.out_layer.ec.weights, \\\n",
    "                                                         rvcapture[1][inx], rvsearch[1][inx], rvbackoff[1][inx], \\\n",
    "                                                         rvmin[1][inx], rvstickup[1][inx], rvstickdown[1][inx])\n",
    "    \n",
    "    ##################\n",
    "    ## Deterministic\n",
    "    ##################\n",
    "    \n",
    "    \n",
    "    #trains one sample\n",
    "    def train_both_layers_det(self, sample, target, k_hidden):        \n",
    "        #initial hidden state is 0\n",
    "        out_prev = torch.ones(self.recurrent_size) * float('inf')\n",
    "        \n",
    "        for inx, data in enumerate(sample):\n",
    "            #concatenate input with the previous output\n",
    "            data = torch.cat((data, out_prev))\n",
    "            # First layer feedforward processing\n",
    "            out1, winner            = self.hidden_layer(data, k_hidden)\n",
    "            \n",
    "            #first layer is regular stdp\n",
    "            self.hidden_layer.ec.weights    = self.hidden_layer.stdp(data, out1, self.hidden_layer.ec.weights, \\\n",
    "                                                                     self.lr_capture, self.lr_backoff, self.lr_search)\n",
    "            \n",
    "            #update previous out\n",
    "            out_prev               = out1 \n",
    "            \n",
    "            #second layer feedforward processing\n",
    "            out2, winner            = self.out_layer(out1, 1)\n",
    "            if self.out_stdp == 'stdp':\n",
    "                self.out_layer.ec.weights = self.out_layer.stdp(out1, out2, self.out_layer.ec.weights, \\\n",
    "                                                                self.lr_capture, self.lr_backoff, self.lr_search)\n",
    "            elif self.out_stdp == 'rstdp':\n",
    "                reward = 0\n",
    "                if winner != -1:\n",
    "                    if target == winner.item():\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        reward = -1\n",
    "                self.out_layer.ec.weights = self.out_layer.stdp(reward, out1, out2, self.out_layer.ec.weights, \\\n",
    "                                                                 self.lr_capture, self.lr_backoff, self.lr_search)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def train_hidden_layer_det(self, sample, target, k_hidden):\n",
    "        #initial hidden state is 0\n",
    "        out_prev = torch.ones(self.recurrent_size) * float('inf')\n",
    "        winner_prev = -1\n",
    "        for inx, data in enumerate(sample):\n",
    "#             if winner_prev != -1:\n",
    "#                 out_prev[winner_prev.item()] += 1\n",
    "            #concatenate input with the previous output\n",
    "            data = torch.cat((data, out_prev))\n",
    "            \n",
    "            #feedforward pass hidden\n",
    "            out, winner            = self.hidden_layer(data, k_hidden)\n",
    "\n",
    "            if self.hidden_stdp == 'stdp' or self.hidden_stdp == 'stdp_tmod' or 'stdp_smod':\n",
    "                #hidden layer is regular stdp\n",
    "                self.hidden_layer.ec.weights    = self.hidden_layer.stdp(data, out, self.hidden_layer.ec.weights, \\\n",
    "                                                                     self.lr_capture, self.lr_capture_min, self.lr_minus, \\\n",
    "                                                                      self.lr_minus_min, self.lr_backoff, self.lr_search, \\\n",
    "                                                                      True, self.input_size)\n",
    "            elif self.hidden_stdp == 'rstdp':\n",
    "                reward = 0\n",
    "                if winner != -1:\n",
    "                    if target == winner.item():\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        reward = -1\n",
    "                self.hidden_layer.ec.weights    = self.hidden_layer.stdp(reward, data, out, self.hidden_layer.ec.weights, \\\n",
    "                                                                     self.lr_capture, self.lr_backoff, self.lr_search, \\\n",
    "                                                                     True, self.input_size)\n",
    "            #update previous out\n",
    "            out_prev               = out\n",
    "            winner_prev            = winner\n",
    "        \n",
    "    def train_output_layer_det(self, sample, target, k_hidden):\n",
    "        #initial hidden state is 0\n",
    "        out_prev    = torch.ones(self.recurrent_size) * float('inf')\n",
    "        for inx, data in enumerate(sample):\n",
    "            #concatenate input with the previous output\n",
    "            data = torch.cat((data, out_prev))\n",
    "            \n",
    "            #feedforward pass hidden\n",
    "            out, winner            = self.hidden_layer(data, k_hidden) \n",
    "            \n",
    "            if out[out != float('inf')].shape[0] > 0:\n",
    "                mval, minx = torch.max(out[out != float('inf')],0)\n",
    "                if mval.item() > 15:\n",
    "                    out[minx.item()] = 15\n",
    "            \n",
    "            #update previous out\n",
    "            out_prev               = out\n",
    "        #feedforward pass out\n",
    "        out_final, winner            = self.out_layer(out, 1)\n",
    "            \n",
    "        if self.out_stdp == 'stdp' or self.out_stdp == 'stdp_tmod':\n",
    "            self.out_layer.ec.weights = self.out_layer.stdp(out, out_final, self.out_layer.ec.weights, \\\n",
    "                                                                self.lr_capture, self.lr_backoff, self.lr_search)\n",
    "            \n",
    "        elif self.out_stdp == 'rstdp' or self.out_stdp == 'rstdp_tmod':\n",
    "            reward = 0\n",
    "            if winner != -1:\n",
    "                if target.item() == winner.item():\n",
    "                        reward = 1\n",
    "                else:\n",
    "                        reward = -1\n",
    "            self.out_layer.ec.weights = self.out_layer.stdp(reward, target.item(), out, out_final, self.out_layer.ec.weights, \\\n",
    "                                                                self.lr_capture, self.lr_backoff, self.lr_search)\n",
    "            \n",
    "             \n",
    "    \n",
    "    def predict(self, test_loader, datasize):\n",
    "        predictions = torch.zeros(datasize)\n",
    "        table = torch.zeros((self.layer2_out_size, self.layer2_out_size))\n",
    "        for inx, (data, label) in enumerate(test_loader):\n",
    "            predicted = predict_sample(self, sample)\n",
    "            predictions[inx] = predicted\n",
    "            if predicted != -1:\n",
    "                table[predicted.item(), label[0]] += 1\n",
    "            \n",
    "        return (table, predictions)\n",
    "    \n",
    "    def predict_sample(self, sample):\n",
    "        #initial hidden state is 0\n",
    "        out_prev = torch.ones(self.recurrent_size) * float('inf')\n",
    "        \n",
    "        for inx, data in enumerate(sample):\n",
    "            #concatenate input with the previous output\n",
    "            data = torch.cat((data, out_prev))\n",
    "            out1, winner            = self.layer1(data)\n",
    "            \n",
    "            #second layer feedforward processing\n",
    "            out2, winner            = self.layer2(out1)\n",
    "            #update previous out\n",
    "            out_prev               = out2 \n",
    "        \n",
    "        return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
